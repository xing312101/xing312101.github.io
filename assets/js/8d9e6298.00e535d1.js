"use strict";(self.webpackChunkxing_life_3=self.webpackChunkxing_life_3||[]).push([[1678],{6551:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>m,frontMatter:()=>i,metadata:()=>a,toc:()=>c});var r=t(4848),o=t(8453);const i={},s="AI",a={id:"technique/zzz_waiting_research/AI-LLM",title:"AI",description:"tokenization",source:"@site/docs/technique/zzz_waiting_research/AI-LLM.md",sourceDirName:"technique/zzz_waiting_research",slug:"/technique/zzz_waiting_research/AI-LLM",permalink:"/docs/technique/zzz_waiting_research/AI-LLM",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"techniqueSidebar",previous:{title:"Waiting Research",permalink:"/docs/technique/zzz_waiting_research/"}},d={},c=[{value:"tokenization",id:"tokenization",level:2},{value:"EMBEDDINGS WORKFLOW",id:"embeddings-workflow",level:2},{value:"tokenizer.py",id:"tokenizerpy",level:2},{value:"model_embeddings.py",id:"model_embeddingspy",level:2},{value:"3 masked_language.py",id:"3-masked_languagepy",level:2},{value:"semantic_index.py",id:"semantic_indexpy",level:2},{value:"BERT for Question Answering.ipynb",id:"bert-for-question-answeringipynb",level:2},{value:"GPT for Instruction Following",id:"gpt-for-instruction-following",level:2},{value:"T5 for Product Reviews",id:"t5-for-product-reviews",level:2}];function l(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"ai",children:"AI"}),"\n",(0,r.jsx)(n.h2,{id:"tokenization",children:"tokenization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'from transformers import AutoTokenizer, AutoModel\n\n# Initializing the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")\nmodel = AutoModel.from_pretrained("distilbert-base-uncased")\n\n# Step 1: Tokenize raw text using the chosen method (word or subword tokenization)\nsentence = "Transformers are quite amazing"\ntokens = tokenizer.tokenize(sentence)\nprint("Tokens:", tokens)\n# Tokens:[\'transformers\', \'are\', \'quite\', \'amazing\']\n\n# Step2: Convert tokens into numerical indices corresponding to model vocabulary\ninput_ids = tokenizer.encode(sentence)\nprint("Input Ids:", input_ids)\n# Input Ids [[101, 19067, 2024, 3243, 6429, 102]]\n\n# Step3: Fetch context-specific embeddings from the model and process through multiple layers\noutput = model(input_ids)\nembeddings = output.last_hidden_state\nprint("Embeddings:", embeddings);\n# Embeddings: tensor([[[-0.0555, 0.1111, ..., -0.3933, 0.2311]], ...])\n\n# Step4: Process/decode output back into text (Here, we use theembeddings directly to illustrate)\ndecoded_output = tokenizer.decode(input_ids[0])\n\nprint("Decoded output:", decoded_output)\n# Decoded output: [CLS] transformers are quite amazing [SEP]\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"embeddings-workflow",children:"EMBEDDINGS WORKFLOW"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'import torch\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")\nmodel = AutoModel.from_pretrained("bert-base-uncased")\n\n# Two sentences with different contexts for the word "rose"\nsentence1 = "She took a rose from the garden."\nsentence2 = "The sun rose in the morning."\n\n# Tokenize sentences to obtain input_ids\ninput_ids1 = tokenizer(sentence1, return_tensors="pt")["input_ids"]\ninput_ids2 = tokenizer(sentence2, return_tensors="pt")["input_ids"]\n\n# Obtain embeddings from the model\nwith torch.no_grad():\n    embeddings1 = model(input_ids1).last_hidden_state\n    embeddings2 = model(input_ids2).last_hidden_state\n\n# Extract the embeddings for the word "rose".\nembedding1 = embeddings1[0][tokenizer.convert_tokens_to_ids("rose")]\nembedding2 = embeddings2[0][tokenizer.convert_tokens_to_ids("rose")]\n\n# Compare the similarity between the two "rose" embeddings through cosine similarity.\ncosine_similarity = torch.nn.CosineSimilarity(dim=0)\nsimilarity = cosine_similarity(embedding1, embedding2)\n\nprint(f"Cosine similarity between the two embeddings:{similarity}")\n# Cosine similarity between the two embeddings: 0.4351\n\n# sentence1 -> "rose" -> 3123 => tensor([-0.1230, 0.6666, ..])\n# sentence2 -> "rose" -> 3123 => tensor([-0.0999, 0.7777, ..])\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"tokenizerpy",children:"tokenizer.py"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'# Import required libraries\nfrom transformers import BertModel, AutoTokenizer\nimport pandas as pd\n\n# Specify the pre-trained model to use: BERT-base-cased\nmodel_name = "bert-base-cased"\n\n# Instantiate the model and tokenizer for the specified pre-trained model\nmodel = BertModel.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Set a sentence for analysis\nsentence = "When life gives you lemons, don\'t make lemonade."\n\n# Tokenize the sentence\ntokens = tokenizer.tokenize(sentence)\n\n# Create a DataFrame with the tokenizer\'s vocabulary\nvocab = tokenizer.vocab\nvocab_df = pd.DataFrame({"token": vocab.keys(), "token_id": vocab.values()})\nvocab_df = vocab_df.sort_values(by="token_id").set_index("token_id")\n\n# Encode the sentence into token_ids using the tokenizer\ntoken_ids = tokenizer.encode(sentence)\n\n# Print the length of tokens and token_ids\nlen(tokens)\nlen(token_ids)\n\n# Access the tokens in the vocabulary DataFrame by index\nvocab_df.iloc[101]\nvocab_df.iloc[102]\n\n# Zip tokens and token_ids (excluding the first and last token_ids for [CLS] and [SEP])\nlist(zip(tokens, token_ids[1:-1]))\n\n# Decode the token_ids (excluding the first and last token_ids for [CLS] and [SEP]) back into the original sentence\ntokenizer.decode(token_ids[1:-1])\n\n# Tokenize the sentence using the tokenizer\'s `__call__` method\ntokenizer_out = tokenizer(sentence)\n\n# Create a new sentence by removing "don\'t " from the original sentence\nsentence2 = sentence.replace("don\'t ", "")\n\n# Tokenize both sentences with padding\ntokenizer_out2 = tokenizer([sentence, sentence2], padding=True)\n\n# Decode the tokenized input_ids for both sentences\ntokenizer.decode(tokenizer_out2["input_ids"][0])\ntokenizer.decode(tokenizer_out2["input_ids"][1])\n'})}),"\n",(0,r.jsx)(n.h2,{id:"model_embeddingspy",children:"model_embeddings.py"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'from transformers import BertModel, AutoTokenizer\nfrom scipy.spatial.distance import cosine\n\nmodel_name = "bert-base-cased"\n\nmodel = BertModel.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\ndef predict(text):\n    encoded_inputs = tokenizer(text, return_tensors="pt")\n\n    return model(**encoded_inputs)[0]\n\n\nsentence1 = "There was a fly drinking from my soup"\nsentence2 = "There is a fly swimming in my juice"\n# sentence2 = "To become a commercial pilot, he had to fly for 1500 hours."\n\ntokens1 = tokenizer.tokenize(sentence1)\ntokens2 = tokenizer.tokenize(sentence2)\n\nout1 = predict(sentence1)\nout2 = predict(sentence2)\n\nemb1 = out1[0:, tokens1.index("fly"), :].detach()\nemb2 = out2[0:, tokens2.index("fly"), :].detach()\n\n# emb1 = out1[0:, 3, :].detach()\n# emb2 = out2[0:, 3, :].detach()\n\n\nemb1.shape\nemb2.shape\n\ncosine(emb1, emb2)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"3-masked_languagepy",children:"3 masked_language.py"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"# Import required libraries\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nfrom scipy.special import softmax\nimport numpy as np\n\n# Specify the pre-trained model to use: BERT-base-cased\nmodel_name = \"bert-base-cased\"\n\n# Instantiate the tokenizer and model for the specified pre-trained model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\n\n# Get the mask token from the tokenizer\nmask = tokenizer.mask_token\n\n# Create a sentence with a mask token to be filled in by the model\nsentence = f\"I want to {mask} pizza for tonight.\"\n\n# Tokenize the sentence\ntokens = tokenizer.tokenize(sentence)\n\n# Encode the sentence using the tokenizer and return the input tensors\nencoded_inputs = tokenizer(sentence, return_tensors='pt')\n\n# Get the model's output for the input tensors\noutputs = model(**encoded_inputs)\n# Detach the logits from the model's output and convert them to numpy arrays\nlogits = outputs.logits.detach().numpy()[0]\n\n# Extract the logits for the mask token\nmask_logits = logits[tokens.index(mask) + 1]\n# Calculate the confidence scores for each possible token using softmax\nconfidence_scores = softmax(mask_logits)\n\n# Print the top 5 predicted tokens and their confidence scores\nfor i in np.argsort(confidence_scores)[::-1][:5]:\n    pred_token = tokenizer.decode(i)\n    score = confidence_scores[i]\n\n    # Print the predicted sentence with the mask token replaced by the predicted token, and the confidence score\n    print(sentence.replace(mask, pred_token), score)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"semantic_indexpy",children:"semantic_index.py"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'# Import required libraries\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# Load the multi_news dataset from Hugging Face and take only the \'test\' split for efficiency\ndataset = load_dataset("multi_news", split="test")\n\n# Convert the test dataset to a pandas DataFrame and take only 2000 random samples\ndf = dataset.to_pandas().sample(2000, random_state=42)\n\n# Load a pre-trained sentence transformer model\nmodel = SentenceTransformer("all-MiniLM-L6-v2")\n\n# Encode each summary in the DataFrame using the sentence transformer model and store the embeddings in a list\npassage_embeddings = list(model.encode(df[\'summary\'].to_list(), show_progress_bar=True))\n\n# Print the shape of the first passage embedding\npassage_embeddings[0].shape\n\n# Declare a query string\nquery = "Find me some articles about technology and artificial intelligence"\n\n# Define a function to find relevant news articles based on a given query\ndef find_relevant_news(query):\n    # Encode the query using the sentence transformer model\n    query_embedding = model.encode(query)\n    # Print the shape of the query embedding\n    query_embedding.shape\n\n    # Calculate the cosine similarity between the query embedding and the passage embeddings\n    similarities = util.cos_sim(query_embedding, passage_embeddings)\n\n    # Find the indices of the top 3 most similar passages\n    top_indicies = torch.topk(similarities.flatten(), 3).indices\n\n    # Get the top 3 relevant passages by slicing the summaries at 200 characters and adding an ellipsis\n    top_relevant_passages = [df.iloc[x.item()][\'summary\'][:200] + "..." for x in top_indicies]\n\n    # Return the top 3 relevant passages\n    return top_relevant_passages\n\n# Find relevant news articles for different queries\nfind_relevant_news("Natural disasters")\nfind_relevant_news("Law enforcement and police")\nfind_relevant_news("Politics, diplomacy and nationalism")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"bert-for-question-answeringipynb",children:"BERT for Question Answering.ipynb"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'# Importing necessary libraries\nimport torch\nfrom transformers import (\n    BertForQuestionAnswering,\n    BertTokenizerFast,\n)\nfrom scipy.special import softmax\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\n\n# Defining the context and the question\ncontext = "The giraffe is a large African hoofed mammal belonging to the genus Giraffa. It is the tallest living terrestrial animal and the largest ruminant on Earth. Traditionally, giraffes were thought to be one species, Giraffa camelopardalis, with nine subspecies. Most recently, researchers proposed dividing them into up to eight extant species due to new research into their mitochondrial and nuclear DNA, as well as morphological measurements. Seven other extinct species of Giraffa are known from the fossil record."\nquestion = "How many giraffe species are there?"\n\n# Defining the model name and loading the tokenizer and the model\nmodel_name = "deepset/bert-base-cased-squad2"\ntokenizer = BertTokenizerFast.from_pretrained(model_name)\nmodel = BertForQuestionAnswering.from_pretrained(model_name)\n\n# Tokenizing the context and the question\ninputs = tokenizer(question, context, return_tensors="pt")\ntokenizer.tokenize(context)\n\n# Running the model and getting the start and end scores\nwith torch.no_grad():\n    outputs = model(**inputs)\nstart_scores, end_scores = softmax(outputs.start_logits)[0], softmax(outputs.end_logits)[0]\n\n# Creating a dataframe with the scores and plotting them\nscores_df = pd.DataFrame({\n    "Token Position": list(range(len(start_scores))) * 2,\n    "Score": list(start_scores) + list(end_scores),\n    "Score Type": ["Start"] * len(start_scores) + ["End"] * len(end_scores),\n})\npx.bar(scores_df, x="Token Position", y="Score", color="Score Type", barmode="group", title="Start and End Scores for Tokens")\n\n# Getting the answer from the model\nstart_idx = np.argmax(start_scores)\nend_idx = np.argmax(end_scores)\nanswer_ids = inputs.input_ids[0][start_idx: end_idx + 1]\nanswer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)\nanswer = tokenizer.convert_tokens_to_string(answer_tokens)\n\n# Part 2\n# Defining a function to predict the answer to a question given a context\ndef predict_answer(context, question):\n    inputs = tokenizer(question, context, return_tensors="pt", truncation=True, max_length=512)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    start_scores, end_scores = softmax(outputs.start_logits)[0], softmax(outputs.end_logits)[0]\n    start_idx = np.argmax(start_scores)\n    end_idx = np.argmax(end_scores)\n    confidence_score = (start_scores[start_idx] + end_scores[end_idx]) /2\n    answer_ids = inputs.input_ids[0][start_idx: end_idx + 1]\n    answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)\n    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n    if answer != tokenizer.cls_token:\n        return answer, confidence_score\n    return None, confidence_score\n\n# Defining a new context and predicting answers to some questions\ncontext = """Coffee is a beverage prepared from roasted coffee beans. Darkly colored, bitter, and slightly acidic, coffee has a stimulating effect on humans, primarily due to its caffeine content. It has the highest sales in the world market for hot drinks.[2][unreliable source?]\n...\n"""\nlen(tokenizer.tokenize(context))\npredict_answer(context, "What is coffee?")\npredict_answer(context, "What are the most common coffee beans?")\npredict_answer(context, "How can I make ice coffee?")\npredict_answer(context[4000:], "How many people are dependent on coffee for their income?")\n\n# Defining a function to chunk sentences\ndef chunk_sentences(sentences, chunk_size, stride):\n    chunks = []\n    num_sentences = len(sentences)\n    for i in range(0, num_sentences, chunk_size - stride):\n        chunk = sentences[i: i + chunk_size]\n        chunks.append(chunk)\n    return chunks\n\nsentences = [\n    "Sentence 1.",\n    "Sentence 2.",\n    "Sentence 3.",\n    "Sentence 4.",\n    "Sentence 5.",\n    "Sentence 6.",\n    "Sentence 7.",\n    "Sentence 8.",\n    "Sentence 9.",\n    "Sentence 10."\n]\n\nchunked_sentences = chunk_sentences(sentences, chunk_size=3, stride=1)\n\n\nquestions = ["What is coffee?", "What are the most common coffee beans?", "How can I make ice coffee?", "How many people are dependent on coffee for their income?"]\n\nanswers = {}\n\nfor chunk in chunked_sentences:\n    context = "\\n".join(chunk)\n    for question in questions:\n        answer, score = predict_answer(context, question)\n\n        if answer:\n            if question not in answers:\n                answers[question] = (answer, score)\n            else:\n                if score > answers[question][1]:\n                    answers[question] = (answer, score)\n\nanswers\n'})}),"\n",(0,r.jsx)(n.h2,{id:"gpt-for-instruction-following",children:"GPT for Instruction Following"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/TheFuzzyScientist/diabloGPT_open-instruct",children:"https://huggingface.co/TheFuzzyScientist/diabloGPT_open-instruct"})}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"from transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments,\n)\nfrom datasets import load_dataset\n\n\ndataset = load_dataset(\"hakurei/open-instruct-v1\", split='train')\ndataset.to_pandas().sample(20)\n\n\ndef preprocess(example):\n    example['prompt'] = f\"{example['instruction']} {example['input']} {example['output']}\"\n\n    return example\n\ndef tokenize_datasets(dataset):\n    tokenized_dataset = dataset.map(lambda example: tokenizer(example['prompt'], truncation=True, max_length=128), batched=True, remove_columns=['prompt'])\n\n    return tokenized_dataset\n\ndataset = dataset.map(preprocess, remove_columns=['instruction', 'input', 'output'])\ndataset =  dataset.shuffle(42).select(range(100000)).train_test_split(test_size=0.1, seed=42)\n\n\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\n\n\nMODEL_NAME = \"microsoft/DialoGPT-medium\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\ntrain_dataset = tokenize_datasets(train_dataset)\ntest_dataset = tokenize_datasets(test_dataset)\n\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntraing_args = TrainingArguments(output_dir=\"models/diablo_gpt\",\n                                num_train_epochs=1,\n                                per_device_train_batch_size=32,\n                                per_device_eval_batch_size=32)\\\n\ntrainer = Trainer(model=model,\n                    args=traing_args,\n                    train_dataset=train_dataset,\n                    eval_dataset=test_dataset,\n                    data_collator=data_collator)\n\ntrainer.train()\n\n# Get the trained checkpoint directly\nmodel = AutoModelForCausalLM.from_pretrained(\"TheFuzzyScientist/diabloGPT_open-instruct\")\n\n\ndef generate_text(prompt):\n    inputs = tokenizer.encode(prompt, return_tensors='pt').to(\"cuda\")\n    outputs = model.generate(inputs, max_length=64, pad_token_id=tokenizer.eos_token_id)\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return generated[:generated.rfind('.')+1]\n\n\ngenerate_text(\"What's the best way to cook chiken breast?\")\n\ngenerate_text(\"Should I invest stocks?\")\n\ngenerate_text(\"I need a place to go for this summer vacation, what locations would you recommend\")\n\ngenerate_text(\"What's the fastest route from NY City to Boston?\")\n\n\n\n"})}),"\n",(0,r.jsx)(n.h2,{id:"t5-for-product-reviews",children:"T5 for Product Reviews"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/TheFuzzyScientist/T5-base_Amazon-product-reviews",children:"https://huggingface.co/TheFuzzyScientist/T5-base_Amazon-product-reviews"})}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"!pip install numpy==1.25.1\n!pip install transformers\n!pip install datasets===2.13.1\n\n# Importing necessary modules\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\nfrom transformers import DataCollatorWithPadding\n\n# Loading the dataset and removing unnecessary columns\ndataset = load_dataset('amazon_us_reviews', 'Electronics_v1_00', split='train')\ndataset = dataset.remove_columns([x for x in dataset.features if x not in ['review_body', 'verified_purchase', 'review_headline', 'product_title', 'star_rating']])\n\n# Filtering the dataset and encoding the 'star_rating' column\ndataset = dataset.filter(lambda x: x['verified_purchase'] and len(x['review_body']) > 100).shuffle(42).select(range(100000))\ndataset = dataset.class_encode_column(\"star_rating\")\n\n# Splitting the dataset into training and testing sets\ndataset = dataset.train_test_split(test_size=0.1, seed=42, stratify_by_column=\"star_rating\")\n\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\n\n# Initializing the tokenizer\nMODEL_NAME = 't5-base'\ntokenizer = T5Tokenizer.from_pretrained('t5-base')|\n\n# Defining the function to preprocess the data\ndef preprocess_data(examples):\n    examples['prompt'] = [f\"review: {example['product_title']}, {example['star_rating']} Stars!\" for example in examples]\n    examples['response'] = [f\"{example['review_headline']} {example['review_body']}\" for example in examples]\n\n    inputs = tokenizer(examples['prompt'], padding='max_length', truncation=True, max_length=128)\n    targets = tokenizer(examples['response'], padding='max_length', truncation=True, max_length=128)\n\n    # Set -100 at the padding positions of target tokens\n    target_input_ids = []\n    for ids in targets['input_ids']:\n        target_input_ids.append([id if id != tokenizer.pad_token_id else -100 for id in ids])\n\n    inputs.update({'labels': target_input_ids})\n    return inputs\n\n# Preprocessing the training and testing datasets\ntrain_dataset = train_dataset.map(preprocess_data, batched=True)\ntest_dataset = test_dataset.map(preprocess_data, batched=True)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Fine-tuning the T5 model\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n\nTRAINING_OUTPUT = \"./models/t5_fine_tuned_reviews\"\ntraining_args = TrainingArguments(\n    output_dir=TRAINING_OUTPUT,\n    num_train_epochs=3,\n    per_device_train_batch_size=12,\n    per_device_eval_batch_size=12,\n    save_strategy='epoch',\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=data_collator,\n)\n\ntrainer.train()\n\n# Saving the model\ntrainer.save_model(TRAINING_OUTPUT)\n\n# Loading the fine-tuned model\nmodel = T5ForConditionalGeneration.from_pretrained(TRAINING_OUTPUT)\n\n# or get it directly trained from here:\n# model = T5ForConditionalGeneration.from_pretrained(\"TheFuzzyScientist/T5-base_Amazon-product-reviews\")\n\n# Defining the function to generate reviews\ndef generate_review(text):\n    inputs = tokenizer(\"review: \" + text, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n    outputs = model.generate(inputs['input_ids'], max_length=128, no_repeat_ngram_size=3, num_beams=6, early_stopping=True)\n    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return summary\n\n# Generating reviews for random products\nrandom_products = test_dataset.shuffle(42).select(range(10))['product_title']\n\nprint(generate_review(random_products[0] + \", 3 Stars!\"))\nprint(generate_review(random_products[1] + \", 5 Stars!\"))\nprint(generate_review(random_products[2] +\n"})})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var r=t(6540);const o={},i=r.createContext(o);function s(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);